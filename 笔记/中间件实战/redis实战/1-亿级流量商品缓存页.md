 



.\redis-server.exe .\redis.windows-service.conf



# 1.redis生产环境启动方案

![image-20210425203616979](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210425203617.png)

![image-20210425203812659](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210425203812.png)





# 2.redis持久化对于生产环境中灾难恢复的意义



1.redis持久化 RDB,AOF区别，各自的特点是什么，适合什么场景

2.redis突然挂掉了，进程死了，或者所在的机器没了，反正遇到灾难性的故障，redis挂了。存放的数据全都没了，很惨，很重要的缓存数据，服务了很多重要的系统和服务。redis会重启，重启之后，需要费很大的劲去恢复redis。redis如果单单存放在内存中，是没有任何办法应对一些灾难性的故障的。





# 3.分析Redis的RDB和AOF两种持久化机制的工作原理

持久化主要是为了灾难恢复，数据恢复，也可以归类到高可用的一个环节中

## RDB

对redis中的数据执行周期性的持久化(每隔一段时间生成redis内存中的数据的一份完整快照)，





## AOF

**AOF文件是存放每条写命令的**

操作系统会有一个os cache，写文件不会直接写入磁盘，会先往os cache中写入，然后到一定时间在从os cache到disf file

AOF是每隔一段时间调用一次操作系统的fsync操作，强制将os cache中的数据刷入磁盘文件中。

redis中的数据是有一定限量的，不可能无限增长，导致AOF文件无限增长。到一定时候redis就会用缓存淘汰算法比如说LRU，删除一些数据

到AOF文件大到一定时候，AOF就会做rewrite(不是基于已存在的AOF文件，而是当时redis的状态)操作，会基于redis内存中的数据，来重新构造一个更小的AOF文件





## 对比

如果同时使用RDB和AOF两种持久化数据，在redis重启的时候或优先使用AOF文件来重构数据，因为数据更加完整



**RDB优点**

1. RDB会生成多个数据文件，每个数据文件都代表了某一时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件，发送到一些远程的安全存储服务上去。
2. 对redis对外提供的读写服务，影响非常下，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘io操作来进行rdb持久化即可
3. 相对AOF持久化来说，直接基于RDB来重启和回复redis更快



**RDB缺点**

1. RDB生成数据快照一般是5分钟，或者更长，这个时候宕机，那么就丢失了5分钟的数据
2. 如果RDB在fork子进程来执行RDB的时候，如果文件特别大，可能会导致服务暂停数毫秒，甚至数秒。所以RDB间隔不能特别大



**AOF优点**

1. aof更好的保护数据不丢失，一般aof会每隔1秒进行一次fsync操作，最多丢失1秒的数据
2. 使用append-only写入，不会有磁盘寻址的开销，写入性能高，不容易被破坏
3. rewrite是对新老文件一起写入，然后进行merge
4. aof是以记录命令的方式进行的，可以查看所有的操作记录，防止flushall的破坏操作





**AOF缺点**

1. AOF文件更大
2. 性能有降低
3. AOF可能有bug，不能恢复出一模一样的数据
4. 数据恢复的时候比较慢，不方便，可能需要自己写脚本



![image-20210426210731824](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210426210739.png)





# 4.redis的RDB持久化配置以及数据恢复实验

配置RDB持久化操作：

~~~bash
save 60 1000
~~~

可以设置多个

每隔60s,如果有超过1000个key发生了变化，就新生成一个dump.rdb文件，就是当前redis内存中的完整数据的快照(snapshotting)，也可以手动调用save 或者 bgsave，同步或异步执行rdb快照生成。



## 关闭测试：

~~~
shutdown
~~~

使用之后，会在关闭直接保存redis数据

~~~~
异常关闭，比如kill -9
~~~~

使用之后，不会保存数据





# 5.深入讲解redis的AOF持久化的各种操作和相关实验

AOF默认是关闭的

~~~bash
appendonly yes
~~~

打开aof持久化机制，在生产环境中一般来说AOF是要打开的，除非说随便丢个几分钟数据都无所谓。

打开AOF之后，redis每次接受到一条命令后，就会写入日志文件中，当然先写入os cache中，然后每隔一段时间 fsync中，如果AOF和RDB都开启了，优先通过AOF文件恢复数据。

**fsync策略有3种**

~~~
always:每次写入一条数据，立即执行fsync
	mysql：内存策略，大量磁盘，qps一般 1 - 2K
	redis：内存，磁盘持久化 qps 单机 一般 1w+ 没问题
everysec:每秒将os cache中的数据fsync到磁盘，一般都这么配置
no:redis仅仅将数据写入os chache就不管了，靠os自己的策略掉用fsync
~~~



## 测试

![image-20210427214935191](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210427214958.png)

![image-20210427214940080](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210427215002.png)

可以看到在appendonly.aof文件中，可以看到刚刚的命令日志，他们其实就是先写入到os cache的，然后1秒后调用fsync写入到aof文件中.

kill _ 9后，数据也恢复了

![image-20210427215236004](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210427215236.png)_



## rewrite

redis中的数据可能是优先的，很多数据可能会自动过期，可能会被用户删除，可能会被redis用缓存清除的算法清理掉。

redis中的数据会不断的淘汰，只有一些常用的数据会被自动保留在redis内存中。

所以一些已经被淘汰的数据会存在于aof文件中，aof文件就一个，会不断的变大，很大。

所以aof会自动在后台每隔一段时间进行rewrite操作，比如日志里面已经存放了100w数据的写日志；redis内存只剩10w数据，基于内存中当前10w数据构建一套最新的日志，覆盖老的aof文件，确保aof文件不会过大，保持跟redis内存数据量一致。

redis2.4之前，还需要手动调用 BGREWRITEAOF 命令执行aof rewrite



**rewrite相关配置**

redis会记录上次rewrite文件的大小，如果超过上次aof文件percentage的百分比大小就进行rewrite

min-size：最小这么大才进行rewrite

~~~bash
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
~~~

比如上次是128mb，如果现在是256mb，就会触发rewrite，但是还要确认min-size。



**rewrite步骤**

~~~
1.redis fork一个子进程
2.子进程基于当前redis内存中的数据，构建一个新的aof文件
3.redis主线程，在收到客户端的写操作之后，在内存中写入操作记录，并写入旧的aof文件
4.子进程写完新数据后，redis主进程也会将内存中的新的日志追加到新的aof文件中
5.用新的aof文件替换旧的
~~~



## aof和rdb同时工作

~~~
如果rdb在执行snapshotting操作，那么redis不会执行aof的rewrite操作。
如果在执行aof rewrite操作， 那么就不会执行rdb snapshotting。
如果在执行rdb snapshotting，用户调用了BGREWRITEAOF命令，那么等rdb操作完了之后就会执行aof操作
如果同时有aof文件和rdb文件，那么redis重启的时候会使用aof文件进行数据恢复
~~~

如果aof有部分数据，rdb也有部分数据。但是redis重启的时候，不会加载rdb中的数据





# 6.在项目中部署redis企业级数据备份方案以及各种踩坑的数据恢复容灾演练

RDB的生成策略，用默认的也差不多。根据实际情况修改

~~~bash
save 60 10000
~~~

AOF一定要打开，fsync：everysec

~~~bash
auto-aof-rewrite-percentage 100 就是当前文件膨胀当超过上次的100%
auto-aof-rewrite-min-size 64mb  根据数据量来定
~~~





## 备份方案

~~~
1.写crontab定时调度脚本做数据备份
2.每小时都copy一份rdb备份，到一个目录中，只保存最近48小时的备份
3.每天都保留一份当日的rdb备份，到一个目录，只保存最近一个月的
4.每次copy的时候，把太旧的备份删除
5.当天晚上前将服务器的所有数据备份，发送一份到远程的云服务上
~~~





### 每小时备份

![image-20210428211719914](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210428211720.png)cp

### 每天备份

![image-20210428211528175](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210428211528.png)







## 数据恢复方案

![image-20210428214422758](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210428214422.png)

![image-20210428215201292](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210428215201.png)





![image-20210428215458857](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210428215458.png)







# 7.redis如何通过读写分离来承载读请求QPS超过10万+？

redis支持高并发的瓶颈：

~~~
单机redis：大概qps大概在上万~几万不等(根据业务操作的复杂性，lua脚本)
~~~



如果要支持超过10万+的并发，应该怎么做？

~~~
读写分离：一般来说，对于缓存，一般来说都是支撑读高并发，写的请求是较少的，可能一秒也就几千
redis 主从
水平扩容，如果qps在增加，就继续增加redis slave节点
~~~



![image-20210429205523987](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210429205531.png)





# 8.redis主从架构(redis replication)

redis主从架构->读写分离架构->可支持水平扩展的读高并发架构

~~~
master 节点：接收写请求
solve  节点：接收读请求
~~~



1、redis采用异步的方式复制数据到slave节点，slave会周期的确认自己每次复制的数据量

2、一个master节点可以有多个slave节点

3、slave节点可以连接其他的slave节点

4、slave节点复制的时候，不会block master节点的工作

5、slave节点复制的时候也不会block自己的查询操作，他会用旧的数据提供服务，复制完成后，删除旧的数据及，然后加载新的数据，这个时候会暂停

6、slave节点主要用来做进行横向扩容，读写分离，扩容的slave节点可以提高读的吞吐量。



![image-20210429210243881](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210429210243.png)

如果使用了主从架构，那么**master节点必须开启持久化**





# 9.redis主从复制原理、断点续传、无磁盘化复制、过期key处理

## 1.主从复制原理

1.当启动一个slave节点的时候他会发送==PSYNC==命令给master节点

- 如果是slave掉线后重新连接master，那么master只会复制给slave部分数据；**部分数据**

- 如果是第一次连接，那么会触发==full resynchronization==  **全量数据**

2.开始==full resynchronization==的时候，master会启动一个后台进程，开始生成一份rdb快照文件，同时会将从客户端收到的所有写命令缓存在内存中。rdb文件生成完毕后，master会将这个rdb文件发送给slave，salve会将收到的文件写入磁盘，然后在加载到内存中，然后master会将缓存的命令发送给salve，salve同步数据

3.salve如果跟master有网络故障，断开了连接，会自动重连，如果有多个salve都来重新连接，只会生成一份rdb文件，发送给所有的salve





## 2.断点续传

如果网络连接断掉了，可以接着从上一次复制停下来的地方，继续复制。

master中存在一个==backlog==，master和salve都会保存一个==replica offset==和==master id==，offset保存在backlog中的。如果master和slave网络连接断掉了，slave会让master从上次的replica offset开始继续复制。如果没有找到对应的offset，会执行==full resynchronization==





## 3.无磁盘化复制

master在内存中直接创建rdb，然后发送给slave，不会在本地磁盘创建。

~~~bash
repl-diskless-sync
repl-diskless-sync-delay # 等待一定时间开始复制
~~~





## 4.过期key处理

salve不会过期key，只会等待master过期key，如果master过期了一个key，或者通过了淘汰算法了一个key，那么就会模拟一个del命令发送给slave





# 10.深入剖析redis replication原理和完整运行流程



## 1.复制的完整流程

1.slave启动，保存了master节点的信息(host 和ip，从redis.conf中读取)，但是没有开始复制

2.slave节点内有一个定时任务，每秒检查是否有新的master节点要连接和复制，如果发现，就跟master建立socket连接

3.salve节点发送ping命令给master

4.口令认证，如果master设置了requirepass，那么slave必须发送master auth 进行认证

5.master第一次是全量复制，将所有数据发送个slave

6.master后续将写操作，同步复制给slave





![image-20210501135405692](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210501135508.png)





## 2.数据同步相关的核心机制

第一次slave连接master的时候是全量复制

~~~
1.master和slave都会维护一个offset
	master和slave都builder不断累加offset
	slave每秒都会上报自己的slave，同时master也会记录各个slave的offset
~~~


~~~
2.backlog
	master节点有一个backlog，默认是1mb大小
	master节点给slave节点复制数据的时候，也会将数据在bcklog中同步一份
backlog主要用来做全量复制后的 中断 的 增量复制
~~~


~~~
3.master run id
	info server，可以看到master run id
	如果根据host + ip定位master，是不靠谱的，如果master重启或者数据发生了变化，那么slave应该根据不同的run id区分，run id 不同就做全量复制
	如果需要不更改run id重启redis，可以使用redis-cli debug reload命令
~~~

~~~
4.psync
	slave节点使用psync从master 进行复制， psync runid offset
	master节点会根据自身情况返回响应信息，可能是FULLRESYNC runid offset触发全量复制，可能是CONTINUE触发增量复制
~~~





## 3.全量复制

1. master执行**bgsave**，在本地生成一分rdb快照文件
2. master将rdb快照文件发送给slave，如果rdb复制时间超过**60s(repl-timeout)**,那么slave就会认为复制失败，可以适当调解这参数
3. 比如说传输一个6g的文件，每秒100mb，就很可能会超过60s
4. master在生成rdb的时候，会将所有新的写命令缓存在内存中，在slave保存完毕rdb文件后，在将新的写命令发送给slave
5. **client-output-buffer-limit slave 256mb 64mb 60**，如果在复制期间，内存缓存区持续消耗超过64mb，或者一次性超过256mb，那么停止复制，复制失败。
6. salve收到rdb文件后，会清空自己的数据，然后保存到磁盘，然后在加载，此时基于旧数据提供服务
7. 如果slave开启了aof，那么会立即执行BGREWRITEAOF，重写aof



## 4.增量复制

1. 如果全量复制过程中，master-slave网络连接断掉，那么slave重新连接master时，会触发增量复制
2. master直接从自己的**backlog**中获取部分丢失的数据，发送给salve，默认backlog是1mb
3. master就是根据slave发送的**psync**中的offset从backlog获取数据的

 



![image-20210501162620481](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210501162625.png)





# 11.在项目中部署redis的读写分离架构（包含节点间认证口令）

gcc安装

~~~bash
yum install gcc -y
sudo yum install -y centos-release-scl
sudo yum install -y devtoolset-8-gcc*


source /opt/rh/devtoolset-8/enable

或者

mv /usr/bin/gcc /usr/bin/gcc-4.8.5

ln -s /opt/rh/devtoolset-8/root/bin/gcc /usr/bin/gcc

mv /usr/bin/g++ /usr/bin/g++-4.8.5

ln -s /opt/rh/devtoolset-8/root/bin/g++ /usr/bin/g++

gcc --version
~~~







## 安装redis slave

~~~bash
wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gz
tar -zxvf tcl8.6.1-src.tar.gz
cd tcl8.6.1/unix/
./configure
make && make install
~~~



~~~bash
wget http://download.redis.io/releases/redis-6.0.6.tar.gz
tar -zxvf redis-6.0.6.tar.gz
cd redis-6.0.6
make && make instal
~~~

1. redis utils目录中，有个**redis_init_scrpi**t脚本
2. 将**redis_init_scrpit**脚本拷贝到/etc/init.d中，将**redis_init_scrpi**t重命名为**redis_6379**,6379代表监听的端口号

~~~bash
cd utils/
cp redis_init_script /etc/init.d/redis_6379
~~~

3. 修改**redis_6379**中的第6行==REDISPORT==，设置为相同的端口号(默认为6379)

4. 创建两个目录:/etc/redis（存放redis的配置文件）,/var/redis/6379（存放redis的持久化文件）

~~~bash
mkdir /etc/redis && mkdir -p /var/redis/6379
~~~

5. 修改redis配置文件(默认在根目录下，redis.conf)，拷贝到/etc/redis目录中，修改名字为6379.conf。

~~~bash
cp ../redis.conf /etc/redis/6379.conf
~~~

6. 修改redis.conf中的配置文件为生产环境

~~~bash
mkdir -p /var/log/redis/6379
vim /etc/redis/6379.conf

bind 0.0.0.0
daemonize yes										# 以守护进程运行
pidfile   /var/run/redis_6379.pid					# redis的pid文件位置
port 6379											# 设置redis监听的端口号
dir /var/redis/6379/								# 设置持久化文件保存的位置
logfile "/var/log/redis/redis-6379.log"
~~~

7. 启动redis,执行

~~~bash
cd /etc/init.d
chmod 777 redis_6379 
./redis_6379 start
~~~

8. 确认redis进程是否启动， ps -ef |grep redis
9. redis随系统启动

~~~bash
chkconfig redis_6379
~~~



![image-20210501171132040](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210501171852.png)

在slave节点中配置

~~~bash
slaveof 192.168.1.1 6379

replicaof 192.168.1.107 6379
~~~

开启强制读写分离

~~~
slave-read-only yes
~~~



## 安装认证

slave

~~~
masterauth like
~~~

master

~~~
requirepass like
~~~



开放端口，如果没有

![image-20210501173404356](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210501173407.png)

~~~bash
systemctl disable firewalld
~~~





## 启动

~~~
/etc/init.d/redis_6379 start && redis-cli
~~~

![image-20210502121748044](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502122335.png)





# 12.redis压测

~~~
redis-benchmark
-c client
-n request
-d data size SET/GET value in bytes
~~~





# 13.redis哨兵模式实战

## 经典的3节点哨兵集群

![image-20210502134609865](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502134611.png)





## 哨兵主备切换的数据丢失问题：异步复制、集群脑裂

![image-20210502140246560](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502140720.png)



**解决异步复制和脑裂导致的数据丢失**

~~~bash
min-slaves-to-wtire 1
min-slaves-max-lag 10
~~~

要求只少有1个slave，数据同步数据的延迟不能超过10s。

如果说一点所有的slave，数据复制和同步的延迟超过了我们所设置的数据，那么master就不会在接收任何请求





## 原理

sdown：主观宕机，如果有一个哨兵节点任务master宕机~

odown：客观宕机，所有哨兵都认为~

~~~
sdown达成条件:如果一个哨兵ping master，超过 is-master-down-after-millisceonds 指定的毫秒数
odown：quorum指定数量的哨兵也认为~ 
~~~



**哨兵之间的互相发现**

![image-20210502142026331](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502142032.png)





**salve配置的自动纠正**

![image-20210502142107337](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502142110.png)





**master节点选举**

![image-20210502142145528](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210502142145528.png)



**其他**

![image-20210502142744260](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502142750.png)





## 配置

![image-20210502143450840](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210502143450840.png)



~~~
mkdir -p /etc/redis/sentinal
mkdir -p /var/redis/sential/5000
mkdir -p /var/log/redis/sentinal/5000
cp ~/redis-6.0.6/sentinel.conf /etc/redis/sentinal/5000.conf

vim /etc/redis/sentinal/5000.conf
port 5000
bind 0.0.0.0
sentinel monitor mymaster 192.168.1.11 6379 2
daemonize yes
logfile "/var/log/redis/sentinal-5000.log"
~~~



## 启动

~~~
redis-sentinel /etc/redis/sentinal/5000.conf
~~~



![image-20210502150447117](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502150547.png)



![image-20210502150502856](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502150514.png)



![image-20210502150508785](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502150512.png)





## 连接哨兵

~~~bash
redis-cli -h localhost -p 26379
redis-cli -h localhost -p 5000
~~~

查看信息

~~~
sentinel master mymaster
sentinel slaves mymaster
sentinel sentinels mymaster
sentinel get-master-addr-by-name mymaster
~~~





## 选举测试

master关闭

![image-20210502151106446](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502151113.png)

sdown->odwn->master选举

![image-20210502151234118](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502151234.png)

结果

![image-20210502151245085](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502151333.png)

重新开启192.168.1.11（最开始的master）

![image-20210502151410336](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210502151410336.png)

变成了slave节点

sentienl的配置(自动编辑)

![image-20210502152328313](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210502152328313.png)





# 14.redis集群简介

**对集群的解释**

![image-20210502154255526](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502160055.png)

**主从和集群的选择**

![image-20210502154351068](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502160054.png)





# 15.数据分布算法：hash + 一致性hash + redis cluster的hash

![image-20210502155622989](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502160051.png)





# 16.在项目中重新搭建一套读写分离 + 高可用 + 多master的redis clust

![](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502160201.png)

~~~
mkdir -p /etc/redis-cluster
mkdir -p /var/log/redis

protected-mode no
bind 0.0.0.0
port 7001
daemonize yes
logfile "/var/log/redis/7001.log"
cluster-enabled yes
cluster-config-file node-7001.conf
cluster-node-timeout 15000
dir "/var/redis/7001"
appendonly yes
appendfsync everysec
~~~

在1，2，3台机器的/etc/redis-cluster/下分分别创建node-700（1-6）.conf

~~~
mkdir -p /var/redis/7001 -6            # 对应
~~~

准备启动脚本

~~~
cp redis_6379 redis_7001 - 6 
~~~

![image-20210502164349830](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210502164349830.png)

**redis 5不用**

~~~
wget https://cache.ruby-lang.org/pub/ruby/2.7/ruby-2.7.3.tar.gz
tar -zxvf ruby-2.7.3.tar.gz
cd ruby-2.7.3
./configure --prefix=/usr/local/ruby --enable-shared
make && make install

vi /etc/profile
export RUBY_HOME=/usr/local/ruby/
export PATH=$RUBY_HOME/bin:$RUBY_HOME/lib:$PATH

source /etc/profile
~~~



创建集群

~~~
redis-cli --cluster create  192.168.1.11:7001 192.168.1.11:7002 192.168.1.12:7003 192.168.1.12:7004 192.168.1.13:7005 192.168.1.13:7006 --cluster-replicas 1

# 查看集群信息
redis-cli -h localhost -p 7001 -c cluster slots | xargs  -n8 | awk '{print $3":"$4"->"$6":"$7}' | sort -nk2 -t ':' | uniq

redis-cli --cluster check 192.168.1.11:7001
三主三从
~~~

![image-20210502204815512](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210502204820.png)





# 17.动手实验：对项目的redis cluster进行多master写入、读写分离、高可用

**redis cluster多master写入测试：**

~~~
	对redis cluster写入数据的时候，可以将请求发送到任意一个master上去执行，但是每个master会计算这个key对应的CRC16的值，然后对16384个hashslot取模，找到对应的hashslot对应的master，
	如果就是本地，就执行，
	如果在其他的master，就会返回一个moved error，告诉你应该去哪个master上去执行。
	每条数据只能存在一个master上，不同的master负责存储不同的数据，分布式的存储
~~~



![image-20210503140202165](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503140258.png)

~~~
redis-cli -c -h localhost -p 7001
使用 上述命令，在底层自动重定向
~~~

![image-20210503141718475](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503141723.png)



**测试master故障切换**

192.168.1.13 7005 当前是slave

![image-20210503142157240](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503142419.png)

对应的master

![image-20210503142131301](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503142417.png)

master 关闭

![image-20210503142245387](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503142415.png)

192.168.1.13 7005 已经切换为master

![image-20210503142302891](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503142413.png)

7002启动后，变成了slave

![image-20210503142410130](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503142411.png)

# 18.redis cluster如何通过master水平扩容来支撑更高的读写吞吐+海量数据

**新启动一个7007节点**
![image-20210503144651739](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210503144651739.png)

添加7007到集群上去

![image-20210503145148587](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210503145148587.png)

重新分配hash slot

![image-20210503145302934](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210503145302934.png)



~~~
redis-cli --cluster add-node 192.168.1.13:7007 192.168.1.11:7001
redis-cli --cluster check localhost:7007
redis-cli --cluster reshard localhost:7007
~~~





# 19.redis cluster的核心原理分析：gossip通信

redis cluster节点之间使用==gossip==协议进行通信。节点之间不断通信，保持整个集群所有节点的数据是完整的。

~~~
更新，有一定延时，降低了压力
可能导致集群的一些操作滞后
~~~

![image-20210503151707183](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503151709.png)

![image-20210503152005445](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503152010.png)



**主要命令**
![image-20210503152151203](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503152153.png)

**ping消息深入**

![image-20210503152317264](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503152319.png)





# 20.redis在实践中的一些常见问题以及优化思路

slave复制的时候，fork子进程

![image-20210503154614968](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210503154614968.png)

aof

![image-20210503154620983](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210503154620983.png)

![image-20210503154708975](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210503154708975.png)

![image-20210503154713598](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503154713.png)

![image-20210503154720271](C:/Users/pdd20/AppData/Roaming/Typora/typora-user-images/image-20210503154720271.png)

![image-20210503155002505](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503155002.png)





# 21.主要架构方案

![image-20210503160604825](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503161959.png)





# 22.最经典的缓存+数据库读写模式-cache aside pattern

~~~
1.读的时候，先读缓存，缓存没有就读数据库，然后保存到缓存中，返回响应
2.更新的时候，先删除缓存，在更新数据库
~~~



**为什么是删除缓存？**
![image-20210503162225668](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503162228.png)





# 23.高并发场景下的缓存 + 数据库双写不一致问题分析与解决方案

## 1.最初级的缓存不一致问题以及解决方案

![image-20210503163604694](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503163606.png)

![image-20210503163456573](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503163514.png)



## 2.比较复杂的读写并发-数据库缓存不一致

![image-20210503164025505](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503164027.png)

![image-20210503163950048](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503163952.png)



## 3.数据库和缓存的更新和读取操作进行异步串行化

![image-20210503164635539](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503164635.png)

![image-20210503164718253](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210503164720.png)



# 24.高并发场景下恐怖的缓存雪崩现象以及导致系统全盘崩溃的灾难性后果

**缓存雪崩**

~~~
1.redis集群崩溃
2.缓存服务大量对redis的请求hang住，占用资源
3.缓存服务大量的请求打到资源服务里面查询mysql，占用资源
4.源头服务因为mysql被打死，也崩溃，对源服务的请求也hang住，占用资源
5.缓存服务大量的资料全部消耗在访问redis和源服务无果，然后自己被拖死，无法提供服务
6.nginx无法访问缓存服务，redis和源服务，只能基于本地缓存提供服务，但是缓存过期后，没有数据 
7.网站崩溃
~~~



![image-20210504213123275](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210504213139.png)





# 25.缓存雪崩的基于事前+事中+事后三个层次的完美解决方案

## 1.事前

应该避免redis彻底挂掉。

~~~
1.使用双机房部署一套 redis cluster，部分机器在这个机房，另一份在别的机房。
2.两套redis cluster，两套之间做一个数据的同步，redis集群是可以搭建成树状的结构的
~~~



## 2.事中

redis cluster已经彻底崩溃了，开始大量无法访问redis

![image-20210505133152473](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210505133206.png)





## 3.事后

![image-20210505133221641](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210505133225.png)

![image-20210505133514943](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210505133517.png)