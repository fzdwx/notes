

# 1.百度贴吧数据爬取

~~~
1.输入贴吧名称
2.输入起始页
3.输入终止页
4.保存到本地
~~~

https://tieba.baidu.com/f?kw=%E8%8D%AF%E6%B0%B4%E5%93%A5

https://tieba.baidu.com/f?kw=%E8%8D%AF%E6%B0%B4%E5%93%A5&ie=utf-8&pn=50

https://tieba.baidu.com/f?kw=%E8%8D%AF%E6%B0%B4%E5%93%A5&ie=utf-8&pn=100

第n页：(n-1) * 50







```python
from urllib import request, parse

import time
import random


class BaiduSpider(object):
    def __init__(self):
        self.url = 'https://tieba.baidu.com/f?kw={}&pn={}'
        self.headers = {'user-agent': 'mozilla/5.0'}

    # 获取页面
    def getPage(self, url, headers):
        req = request.Request(url=url, headers=headers)
        res = request.urlopen(req)
        html = res.read().decode("utf-8")
        return html

    # 提取数据
    def parseData(self):
        pass

    # 保存数据
    def saveData(self, html, fileName):
        with open(fileName, 'w', encoding='utf-8') as f:
            f.write(html)

    # 主函数
    def start(self):
        tieBaName = input("请输入贴吧名字：")
        startPage = int(input("请输入起始页："))
        endPage = int(input("请输入终止页："))

        for page in range(startPage, endPage + 1):
            pn = (page - 1) * 50
            kw = parse.quote(tieBaName)
            url = self.url.format(kw, pn)
            html = self.getPage(url, self.headers)
            fileName = "{}-第{}页.html".format(tieBaName, page)
            self.saveData(html, fileName)

            print("第{}页爬取成功".format(page))

            time.sleep(random.randint(0, 3))  # 随机休眠


if __name__ == '__main__':
    spider = BaiduSpider()
    spider.start()
```



2.作业

![image-20210203113517600](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210203113524.png)







# 2.csv模块



```python
import csv

with open("text.csv", "w", encoding="utf-8") as f:
    w = csv.writer(f)
    # 写一行
    w.writerow(["like", 18])
    # 写多行
    w.writerow([(),(),(),()])
```





# 3.猫眼TOP100数据爬取并写入csv格式的问题

```python
# - - - - - - - - - - - 
# @author like
# @since 2021-02-03 11:38
# @email 980650920@qq.com
#
from urllib import request, parse
import time
import random
import re
import csv

"""
      <div class="board-item-content">
              <div class="movie-item-info">
              <p class="name"><a href="/films/1297" title="肖申克的救赎" data-act="boarditem-click" data-val="{movieId:1297}">肖申克的救赎</a></p>
              <p class="star">
                主演：蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿
              </p>
              <p class="releasetime">上映时间：1994-09-10(加拿大)</p>    </div>
             <div class="movie-item-number score-num">
             <p class="score"><i class="integer">9.</i><i class="fraction">5</i></p>        
    </div>
"""
fileName = "猫眼-Top100数据.csv"  # 保存文件的名字

url = "https://maoyan.com/board/4?offset={}"  # 请求的url

userAgentList = [
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50",
    "Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 2.0.50727; SLCC2; .NET "
    "CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C; Tablet PC 2.0; .NET4.0E) "
]
# 随机选择一个
randomUserAgent = random.choice(userAgentList)

# 请求头
header = {
    "user-agent": randomUserAgent
}

# 正则表达式
p = re.compile(
    '.*?title="(.*?)".*?</a></p>\D*<p class="star">(.*?\D*?)</p>\D*?<p class="releasetime">(.*?)</p>'
)
# p = re.compile('<div class="movie-item-info">.*?title=" (.*?)"*?class="star">(.*?)</p>*?releasetime">(.*?)</p>')
filmList = []
with open(fileName, 'a', encoding="utf-8") as  f:
    w = csv.writer(f)
    for no in range(1, 11):
        offset = (no - 1) * 10
        finUrl = url.format(offset)
        print(finUrl)

        req = request.Request(finUrl, headers=header)
        res = request.urlopen(req)

        html = res.read().decode('utf-8')

        movieList = p.findall(html)
        for info in movieList:
            data = (info[0].strip(), info[1].strip(), info[2].strip())
            filmList.append(data)
    # w.writerows(filmList)
print(filmList)
    # time.sleep(random.randint(0, 2))  # 随机休眠
```





# 4.电影天堂爬取二级页面.py

```java
# - - - - - - - - - - - 
# @author like
# @since 2021-02-05 10:42
# @email 980650920@qq.com
#
import random
import re
from urllib import request
from randomAgent import *
import time


class FilmSkyData(object):
    p1 = re.compile('<table width="100%".*?<td height="26">.*?<a href="(.*?)".*?>(.*?)</a>', re.S)
    p2 = re.compile('<a href="(.*?)" target="_blank"><strong>')

    def __init__(self):
        self.url = 'https://www.dytt8.net/html/gndy/dyzz/list_23_{}.html'

    # 获取页面数据
    def getPage(self, url):
        req = request.Request(
            url=url,
            headers=getHeaders()
        )
        res = request.urlopen(req)
        html = res.read().decode('gbk', 'ignore')
        return html

    # 解析提取数据
    def parsePage(self, html):

        fileList = self.p1.findall(html)
        for f in fileList:
            fileName = f[1]
            fileLink = f[0]

            # 打开二级页面
            h2Url = 'https://www.dytt8.net' + fileLink
            h2 = self.getPage(h2Url)
            downloadLink = self.p2.findall(h2)
            print({
                'name': fileName,
                '磁力链接地址': downloadLink
            })

    def start(self):
        for no in range(1, 11):
            h1Url = self.url.format(no)
            h1 = self.getPage(h1Url)
            self.parsePage(h1)


# f = FilmSkyData()
# f.start()
if __name__ == '__main__':
    s = FilmSkyData()
    s.start()
```



# 5.lxml

## xpath:

![image-20210206103913762](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210206103920.png)

![image-20210206103933952](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210206103934.png)

![image-20210206103938248](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210206103938.png)

![image-20210206111100910](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210206111100.png)





## lxml

![image-20210206112212469](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210206112212.png)





```python
from lxml import etree

html = """
<div class="main-nav" data-sudaclick="blk_mainnav">
   <div class="nav-mod-1">
   <ul>
      <li><a href="http://news.sina.com.cn/" target="_blank"><b>新闻</b></a></li>
      <li><a href="http://mil.news.sina.com.cn/" target="_blank">军事</a></li>
      <li><a href="https://news.sina.com.cn/china/" target="_blank">国内</a></li>
      <li><a href="http://news.sina.com.cn/world/" target="_blank">国际</a></li>
   </ul>
   <ul>
      <li><a href="http://finance.sina.com.cn/" target="_blank"><b>财经</b></a></li>
      <li><a href="http://finance.sina.com.cn/stock/" target="_blank">股票</a></li>
      <li><a href="http://finance.sina.com.cn/fund/" target="_blank">基金</a></li>
      <li><a href="http://finance.sina.com.cn/forex/" target="_blank">外汇</a></li>
   </ul>
   <ul>
      <li><a href="http://tech.sina.com.cn/" target="_blank"><b>科技</b></a></li>
      <li><a href="http://mobile.sina.com.cn/" target="_blank">手机</a></li>
      <li><a href="http://tech.sina.com.cn/discovery/" target="_blank">探索</a></li>
      <li><a href="http://zhongce.sina.com.cn/" target="_blank">众测</a></li>
   </ul>
</div>
"""
parseHtml = etree.HTML(html)
titleList = parseHtml.xpath('//a/text()')
urlList = parseHtml.xpath('//a/@href')
print(titleList)
print(urlList)
```





![image-20210206114325715](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210206114325.png)



# 6.链家二手房案例

```java
from src.randomAgent import *
import requests
from lxml import etree
import time
import random


class LianJiaSpider(object):
    def __init__(self):
        self.url = "https://bj.lianjia.com/ershoufang/pg{}/"
        self.headers = getHeaders()

    def getPage(self, url):
        res = requests.get(url=url, headers=self.headers)
        res.encoding = 'utf-8'
        html = res.text
        return html

    def parsePage(self, html):
        houseMap = {}
        p = etree.HTML(html)
        liList = p.xpath('//ul[@class="sellListContent"]/li[@data-lj_action_source_type="链家_PC_二手列表页卡片"]')
        for i in liList:
            # 名称
            houseMap['houseName'] = i.xpath('.//a[@data-el="region"]/text()')[0].strip()
            # 总价
            houseMap['totalPrice'] = i.xpath('.//div[@class="totalPrice"]/span/text()')[0].strip()
            # 单价
            houseMap['unitPrice'] = i.xpath('.//div[@class="unitPrice"]/@data-price')[0].strip()
            print(houseMap)

    def start(self):
        for pg in range(1, 11):
            u = self.url.format(pg)
            print(u)
            # 1.获取页面
            html = self.getPage(u)
            # 2.解析页面
            self.parsePage(html)
            time.sleep(random.uniform(0, 1))


if __name__ == '__main__':
    s = LianJiaSpider()
    s.start()
```







# 7.selenium使用

## 下载

http://chromedriver.storage.googleapis.com/index.html?path=88.0.4324.96/

![image-20210214113407947](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210214113415.png)

![image-20210214122749959](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210214122750.png)



## 初步使用

![image-20210214124251536](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210214124251.png)

![image-20210214124042429](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210214124042.png)



## chrome 设置无界面模式

```python
from  selenium import webdriver

op = webdriver.ChromeOptions()
op.add_argument('--headless')

chrome = webdriver.Chrome(options=op)
chrome.get('http://www.baidu.com')
chrome.save_screenshot('baidu.png')
```





# 8.scrapy 



## 安装

~~~python
python -m pip install Scrapy
~~~

## 创建项目

scrapy startproject day08

~~~bash
D:\python\project\python-spider-study\src>scrapy startproject day08
New Scrapy project 'day08', using template directory 'd:\python\sdk\lib\site-packages\scrapy\templates\project', created in:
    D:\python\project\python-spider-study\src\day08

You can start your first spider with:
    cd day08
    scrapy genspider example example.com
~~~

![image-20210216104529634](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210216104536.png)







![image-20210216104610956](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210216104611.png)





## 爬取百度

scrapy genspider 

![image-20210216105056979](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210216105057.png)

## 启动项目

![image-20210216105557887](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210216105557.png)

``` 
scrapy crawl
```



## 过程

![image-20210218085110441](https://gitee.com/likeloveC/picture_bed/raw/master/img/8.26/20210218085117.png)